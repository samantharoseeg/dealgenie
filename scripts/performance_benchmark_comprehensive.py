#!/usr/bin/env python3
"""
DealGenie Comprehensive Performance Benchmark Suite
CodeRabbit: Please review this performance measurement and optimization system

Provides accurate, repeatable performance measurements with:
- p50/p95 metrics across multiple runs
- Hardware specifications capture
- Statistical analysis of performance variation
- Regression test thresholds
"""

import sys
import os
import time
import statistics
import platform
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from features.csv_feature_matrix import CSVFeatureMatrix
from scoring.engine import calculate_score

def get_hardware_specs():
    """Capture hardware specifications for benchmark context."""
    try:
        return {
            "platform": platform.platform(),
            "processor": platform.processor() or platform.machine(),
            "cpu_cores": os.cpu_count() or "Unknown",
            "python_version": platform.python_version(),
            "architecture": platform.architecture()[0]
        }
    except Exception as e:
        return {"error": str(e), "platform": platform.platform()}

def benchmark_single_property_scoring(apn: str, template: str, iterations: int = 10):
    """Benchmark scoring performance for a single property across multiple iterations."""
    feature_matrix = CSVFeatureMatrix()
    times = []
    
    print(f"Benchmarking APN {apn} with {template} template ({iterations} iterations)...")
    
    for i in range(iterations):
        start_time = time.perf_counter()
        
        # Extract features
        features = feature_matrix.get_feature_matrix(apn)
        if not features:
            return None
            
        # Score property
        result = calculate_score(features, template)
        
        end_time = time.perf_counter()
        iteration_time = end_time - start_time
        times.append(iteration_time)
        
        print(f"  Iteration {i+1}: {iteration_time:.4f}s")
    
    return times

def benchmark_feature_extraction(apns: list, iterations: int = 3):
    """Benchmark feature extraction performance across multiple properties."""
    feature_matrix = CSVFeatureMatrix()
    times = []
    
    print(f"Benchmarking feature extraction for {len(apns)} properties ({iterations} iterations)...")
    
    for iteration in range(iterations):
        start_time = time.perf_counter()
        
        successful_extractions = 0
        for apn in apns:
            features = feature_matrix.get_feature_matrix(apn)
            if features and len(features) >= 40:  # Expect 44+ features
                successful_extractions += 1
        
        end_time = time.perf_counter()
        iteration_time = end_time - start_time
        times.append(iteration_time)
        
        parcels_per_second = len(apns) / iteration_time
        print(f"  Iteration {iteration+1}: {iteration_time:.4f}s ({parcels_per_second:.1f} parcels/sec, {successful_extractions}/{len(apns)} successful)")
    
    return times, successful_extractions

def analyze_performance_statistics(times: list, operation_name: str):
    """Analyze performance statistics and return metrics."""
    if not times:
        return None
        
    mean_time = statistics.mean(times)
    median_time = statistics.median(times)
    p95_time = sorted(times)[int(0.95 * len(times))] if len(times) > 1 else times[0]
    std_dev = statistics.stdev(times) if len(times) > 1 else 0
    
    print(f"\nðŸ“Š {operation_name} Performance Statistics:")
    print(f"   Mean: {mean_time:.4f}s")
    print(f"   Median (p50): {median_time:.4f}s") 
    print(f"   95th percentile (p95): {p95_time:.4f}s")
    print(f"   Standard deviation: {std_dev:.4f}s")
    print(f"   Min: {min(times):.4f}s, Max: {max(times):.4f}s")
    
    return {
        "mean": mean_time,
        "median": median_time,
        "p95": p95_time,
        "std_dev": std_dev,
        "min": min(times),
        "max": max(times),
        "sample_size": len(times)
    }

def main():
    """Run comprehensive performance benchmark suite."""
    print("ðŸš€ DealGenie Comprehensive Performance Benchmark")
    print("=" * 60)
    
    # Capture hardware specifications
    hw_specs = get_hardware_specs()
    print("ðŸ’» Hardware Specifications:")
    for key, value in hw_specs.items():
        print(f"   {key}: {value}")
    print()
    
    # Test APNs for benchmarking
    test_apns = [
        "4306026007",  # Known working APN from validation
        "5483019004",  # Another validated APN
        "2353027012",  # Third validated APN  
        "4255016008",  # From bootstrap results
        "2224032011",  # From bootstrap results
    ]
    
    templates = ["multifamily", "residential", "commercial", "industrial", "retail"]
    
    # 1. Single Property Scoring Benchmark
    print("ðŸŽ¯ Single Property Scoring Performance")
    print("-" * 40)
    
    single_property_results = {}
    for template in templates:
        times = benchmark_single_property_scoring(test_apns[0], template, iterations=10)
        if times:
            stats = analyze_performance_statistics(times, f"Single Property ({template})")
            single_property_results[template] = stats
    
    # 2. Feature Extraction Benchmark  
    print("\nðŸ” Feature Extraction Performance")
    print("-" * 40)
    
    extraction_times, successful = benchmark_feature_extraction(test_apns, iterations=5)
    extraction_stats = analyze_performance_statistics(extraction_times, "Feature Extraction")
    
    if extraction_stats:
        avg_parcels_per_second = len(test_apns) / extraction_stats["median"]
        print(f"   ðŸ“ˆ Median Throughput: {avg_parcels_per_second:.1f} parcels/second")
        print(f"   âœ… Success Rate: {successful}/{len(test_apns)} properties")
    
    # 3. End-to-End Pipeline Performance
    print("\nðŸ—ï¸ End-to-End Pipeline Performance")  
    print("-" * 40)
    
    pipeline_times = []
    for i in range(3):
        start_time = time.perf_counter()
        
        feature_matrix = CSVFeatureMatrix()
        pipeline_successful = 0
        
        for apn in test_apns:
            features = feature_matrix.get_feature_matrix(apn)
            if features:
                for template in ["multifamily", "commercial"]:  # Test 2 templates
                    result = calculate_score(features, template)
                    if result and result.get("score") is not None:
                        pipeline_successful += 1
        
        end_time = time.perf_counter()
        pipeline_time = end_time - start_time
        pipeline_times.append(pipeline_time)
        
        operations = len(test_apns) * 2  # 2 templates per APN
        ops_per_second = operations / pipeline_time
        print(f"  Pipeline {i+1}: {pipeline_time:.4f}s ({ops_per_second:.1f} operations/sec, {pipeline_successful}/{operations} successful)")
    
    pipeline_stats = analyze_performance_statistics(pipeline_times, "End-to-End Pipeline")
    
    # 4. Performance Summary and Regression Thresholds
    print("\nðŸ“‹ Performance Summary & Regression Thresholds")
    print("=" * 60)
    
    if extraction_stats and pipeline_stats:
        median_throughput = len(test_apns) / extraction_stats["median"]
        
        print(f"ðŸŽ¯ PRIMARY METRICS (for README):")
        print(f"   Feature Extraction: {median_throughput:.1f} parcels/second (median)")
        print(f"   Single Property Scoring: {single_property_results.get('multifamily', {}).get('median', 0)*1000:.1f}ms (median)")
        print(f"   End-to-End Pipeline: {len(test_apns)*2/pipeline_stats['median']:.1f} operations/second")
        
        print(f"\nâš ï¸  REGRESSION TEST THRESHOLDS:")
        print(f"   Feature extraction must be > {median_throughput * 0.9:.1f} parcels/second")
        print(f"   Single property scoring must be < {single_property_results.get('multifamily', {}).get('p95', 0)*1000*1.1:.0f}ms (p95)")
        print(f"   Pipeline throughput must be > {len(test_apns)*2/pipeline_stats['p95']*0.9:.1f} operations/second")
        
        print(f"\nðŸ”§ RECOMMENDED PERFORMANCE LANGUAGE:")
        print(f"   'Processes {median_throughput:.0f}+ parcels/second with median latency under {single_property_results.get('multifamily', {}).get('median', 0)*1000:.0f}ms'")
        
    print(f"\nâœ… Benchmark completed on {hw_specs.get('platform', 'Unknown')}")
    print(f"   Python {hw_specs.get('python_version', 'Unknown')} | {hw_specs.get('cpu_cores', 'Unknown')} cores | {hw_specs.get('memory_gb', 'Unknown')}GB RAM")

if __name__ == "__main__":
    main()