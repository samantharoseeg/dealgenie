#!/usr/bin/env python3
"""
Performance benchmark script to test DealGenie scoring throughput on real LA County data.
Simple version using only built-in Python libraries (no pandas required).
"""

import time
import subprocess
import statistics
import json
import sys
import os
import csv
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

class DealGeniePerformanceBenchmark:
    def __init__(self, csv_path: str = "scraper/la_parcels_complete_merged.csv"):
        self.csv_path = csv_path
        self.results = {}
        self.apns = []
        
    def load_test_apns(self, sample_size: int = 1000):
        """Load APNs for testing."""
        print(f"Loading {sample_size} test APNs from {self.csv_path}...")
        
        apns = []
        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                headers = next(reader)  # Skip headers
                
                # Find APN column (should be first column)
                apn_col = 0
                for i, header in enumerate(headers):
                    if header.lower() == 'apn':
                        apn_col = i
                        break
                
                row_count = 0
                for row in reader:
                    if len(row) > apn_col and row[apn_col]:
                        apns.append(row[apn_col])
                        row_count += 1
                        if row_count >= sample_size * 2:  # Get extra for selection
                            break
                
        except Exception as e:
            print(f"Error reading CSV: {e}")
            return 0
        
        # Get unique APNs and sample randomly
        unique_apns = list(set(apns))
        self.apns = random.sample(unique_apns, min(sample_size, len(unique_apns)))
        
        print(f"Loaded {len(self.apns)} unique APNs for testing")
        return len(self.apns)
    
    def score_single_apn(self, apn: str, template: str = "multifamily") -> dict:
        """Score a single APN and measure performance."""
        start_time = time.time()
        
        try:
            # Run the scoring command
            cmd = ["python3", "cli/dg_score.py", "score", "--template", template, "--apn", str(apn)]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            end_time = time.time()
            duration = end_time - start_time
            
            success = result.returncode == 0
            
            return {
                "apn": apn,
                "template": template,
                "duration": duration,
                "success": success,
                "stdout": result.stdout if success else "",
                "stderr": result.stderr if not success else "",
                "returncode": result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                "apn": apn,
                "template": template,
                "duration": 30.0,  # Timeout duration
                "success": False,
                "stdout": "",
                "stderr": "Timeout after 30 seconds",
                "returncode": -1
            }
        except Exception as e:
            return {
                "apn": apn,
                "template": template,
                "duration": time.time() - start_time,
                "success": False,
                "stdout": "",
                "stderr": str(e),
                "returncode": -2
            }
    
    def benchmark_sequential(self, test_apns: list, template: str = "multifamily") -> dict:
        """Benchmark sequential processing."""
        print(f"\nüîÑ Running sequential benchmark with {len(test_apns)} APNs...")
        
        start_time = time.time()
        results = []
        
        for i, apn in enumerate(test_apns, 1):
            print(f"  Processing APN {i}/{len(test_apns)}: {apn}")
            result = self.score_single_apn(apn, template)
            results.append(result)
            
            if i % 5 == 0:  # Progress update every 5 APNs
                elapsed = time.time() - start_time
                rate = i / elapsed
                print(f"    Progress: {i}/{len(test_apns)} APNs, Rate: {rate:.2f} parcels/sec")
        
        total_time = time.time() - start_time
        successful_results = [r for r in results if r['success']]
        
        return {
            "method": "sequential",
            "total_apns": len(test_apns),
            "successful_apns": len(successful_results),
            "failed_apns": len(test_apns) - len(successful_results),
            "total_time": total_time,
            "avg_time_per_apn": total_time / len(test_apns),
            "parcels_per_second": len(test_apns) / total_time,
            "successful_parcels_per_second": len(successful_results) / total_time if total_time > 0 else 0,
            "success_rate": len(successful_results) / len(test_apns) if test_apns else 0,
            "individual_times": [r['duration'] for r in successful_results],
            "template": template,
            "detailed_results": results
        }
    
    def benchmark_parallel(self, test_apns: list, template: str = "multifamily", max_workers: int = 4) -> dict:
        """Benchmark parallel processing."""
        print(f"\n‚ö° Running parallel benchmark with {len(test_apns)} APNs (workers: {max_workers})...")
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_apn = {executor.submit(self.score_single_apn, apn, template): apn 
                           for apn in test_apns}
            
            # Collect results as they complete
            for i, future in enumerate(as_completed(future_to_apn), 1):
                result = future.result()
                results.append(result)
                
                if i % 5 == 0:  # Progress update every 5 completions
                    elapsed = time.time() - start_time
                    rate = i / elapsed
                    print(f"    Progress: {i}/{len(test_apns)} APNs, Rate: {rate:.2f} parcels/sec")
        
        total_time = time.time() - start_time
        successful_results = [r for r in results if r['success']]
        
        return {
            "method": "parallel",
            "max_workers": max_workers,
            "total_apns": len(test_apns),
            "successful_apns": len(successful_results),
            "failed_apns": len(test_apns) - len(successful_results),
            "total_time": total_time,
            "avg_time_per_apn": total_time / len(test_apns),
            "parcels_per_second": len(test_apns) / total_time,
            "successful_parcels_per_second": len(successful_results) / total_time if total_time > 0 else 0,
            "success_rate": len(successful_results) / len(test_apns) if test_apns else 0,
            "individual_times": [r['duration'] for r in successful_results],
            "template": template,
            "detailed_results": results
        }
    
    def analyze_performance(self, benchmark_result: dict) -> dict:
        """Analyze performance characteristics."""
        times = benchmark_result['individual_times']
        
        if not times:
            return {"error": "No successful timings to analyze"}
        
        analysis = {
            "timing_stats": {
                "mean": statistics.mean(times),
                "median": statistics.median(times),
                "min": min(times),
                "max": max(times),
                "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
            },
            "performance_metrics": {
                "target_rate": 300,  # parcels/sec target
                "actual_rate": benchmark_result['successful_parcels_per_second'],
                "target_met": benchmark_result['successful_parcels_per_second'] >= 300,
                "efficiency": benchmark_result['success_rate'],
            },
            "bottleneck_analysis": {
                "slow_parcels_count": len([t for t in times if t > 5.0]),  # > 5 seconds
                "very_slow_parcels_count": len([t for t in times if t > 10.0]),  # > 10 seconds
                "timeout_count": len([r for r in benchmark_result['detailed_results'] 
                                    if not r['success'] and 'Timeout' in r['stderr']]),
            }
        }
        
        # Performance assessment
        rate = benchmark_result['successful_parcels_per_second']
        if rate >= 300:
            analysis['assessment'] = "‚úÖ EXCELLENT - Meets production target (300+ parcels/sec)"
        elif rate >= 200:
            analysis['assessment'] = "üü° GOOD - Close to target, minor optimization needed"
        elif rate >= 100:
            analysis['assessment'] = "üü† MODERATE - Significant optimization required"
        else:
            analysis['assessment'] = "üî¥ POOR - Major performance issues, requires investigation"
        
        return analysis
    
    def run_comprehensive_benchmark(self, sample_sizes: list = [25, 50, 100]):
        """Run comprehensive benchmark with different sample sizes."""
        print("="*80)
        print("üöÄ DEALGENIE COMPREHENSIVE PERFORMANCE BENCHMARK")
        print("="*80)
        
        # Load APNs
        max_size = max(sample_sizes)
        loaded_count = self.load_test_apns(max_size)
        
        if loaded_count < max_size:
            print(f"‚ö†Ô∏è  Warning: Only {loaded_count} APNs available, adjusting sample sizes")
            sample_sizes = [s for s in sample_sizes if s <= loaded_count]
        
        all_results = {}
        
        for size in sample_sizes:
            print(f"\n{'='*60}")
            print(f"üìä TESTING WITH {size} PARCELS")
            print(f"{'='*60}")
            
            test_apns = random.sample(self.apns, size)
            
            # Sequential benchmark
            seq_result = self.benchmark_sequential(test_apns, template="multifamily")
            seq_analysis = self.analyze_performance(seq_result)
            
            # Parallel benchmark (if more than 5 APNs)
            par_result = None
            par_analysis = None
            if size >= 5:
                par_result = self.benchmark_parallel(test_apns, template="multifamily", max_workers=4)
                par_analysis = self.analyze_performance(par_result)
            
            all_results[f"sample_{size}"] = {
                "sequential": {
                    "results": seq_result,
                    "analysis": seq_analysis
                },
                "parallel": {
                    "results": par_result,
                    "analysis": par_analysis
                } if par_result else None
            }
            
            # Print results
            self.print_benchmark_summary(size, seq_result, seq_analysis, par_result, par_analysis)
        
        # Save results
        timestamp = int(time.time())
        results_file = f"performance_benchmark_results_{timestamp}.json"
        
        # Convert to JSON-serializable format
        json_results = {}
        for key, value in all_results.items():
            json_results[key] = self._convert_to_json_serializable(value)
        
        with open(results_file, 'w') as f:
            json.dump(json_results, f, indent=2)
        
        print(f"\nüíæ Results saved to: {results_file}")
        
        # Final summary
        self.print_final_summary(all_results)
        
        return all_results
    
    def _convert_to_json_serializable(self, obj):
        """Convert objects to JSON-serializable format."""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(v) for v in obj]
        elif obj is None:
            return None
        else:
            return obj
    
    def print_benchmark_summary(self, size: int, seq_result: dict, seq_analysis: dict, 
                              par_result: dict = None, par_analysis: dict = None):
        """Print summary of benchmark results."""
        print(f"\nüìà RESULTS FOR {size} PARCELS:")
        print(f"   Sequential: {seq_result['successful_parcels_per_second']:.1f} parcels/sec "
              f"({seq_result['success_rate']*100:.1f}% success)")
        
        if par_result:
            print(f"   Parallel:   {par_result['successful_parcels_per_second']:.1f} parcels/sec "
                  f"({par_result['success_rate']*100:.1f}% success)")
        
        print(f"   Assessment: {seq_analysis['assessment']}")
    
    def print_final_summary(self, all_results: dict):
        """Print final performance summary."""
        print(f"\n{'='*80}")
        print("üéØ FINAL PERFORMANCE SUMMARY")
        print("="*80)
        
        # Find best performance
        best_rate = 0
        best_config = ""
        
        for size, results in all_results.items():
            seq_rate = results['sequential']['results']['successful_parcels_per_second']
            if seq_rate > best_rate:
                best_rate = seq_rate
                best_config = f"{size.replace('sample_', '')} parcels (sequential)"
            
            if results['parallel'] and results['parallel']['results']:
                par_rate = results['parallel']['results']['successful_parcels_per_second']
                if par_rate > best_rate:
                    best_rate = par_rate
                    best_config = f"{size.replace('sample_', '')} parcels (parallel)"
        
        print(f"üèÜ Best Performance: {best_rate:.1f} parcels/sec with {best_config}")
        print(f"üéØ Target: 300 parcels/sec")
        print(f"üìä Status: {'‚úÖ TARGET MET' if best_rate >= 300 else 'üî¥ OPTIMIZATION NEEDED'}")
        
        print(f"\nüí° Recommendations:")
        if best_rate >= 300:
            print("   ‚Ä¢ Current performance meets production requirements")
            print("   ‚Ä¢ Consider load testing with larger datasets")
        elif best_rate >= 200:
            print("   ‚Ä¢ Performance is close to target")
            print("   ‚Ä¢ Consider parallel processing optimization")
            print("   ‚Ä¢ Review slow parcels for common patterns")
        else:
            print("   ‚Ä¢ Major performance optimization required")
            print("   ‚Ä¢ Investigate scoring algorithm bottlenecks")
            print("   ‚Ä¢ Consider caching frequently accessed data")
            print("   ‚Ä¢ Profile memory usage and I/O operations")

if __name__ == "__main__":
    # Check if CSV file exists
    csv_path = "scraper/la_parcels_complete_merged.csv"
    if not os.path.exists(csv_path):
        print(f"‚ùå Error: CSV file not found at {csv_path}")
        print("Please ensure the LA County parcel data is available")
        sys.exit(1)
    
    # Initialize benchmark
    benchmark = DealGeniePerformanceBenchmark(csv_path)
    
    # Run comprehensive benchmark with smaller sizes for quick testing
    sample_sizes = [5, 10, 25]  # Start small for initial testing
    results = benchmark.run_comprehensive_benchmark(sample_sizes)
    
    print(f"\nüéâ Benchmark completed!")
    print(f"üìÅ Check the JSON results file for detailed analysis")